{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial : Creating a Simple Nearest Neighbor Classifier from scratch\n",
    "\n",
    "This is based on the K Nearest Neighbours Classifier which is a useful supervised machine learning classification algorithm. But really, we want to know what happens under the hood, i.e. when we call the KNNClassifier( ) from scikit learn, what really happens?\n",
    "\n",
    "The scope of this tutorial is to build a simple Classifier from SCRATCH! \n",
    "But there are a few points to consider :\n",
    "- To keep the time to a minimum, I will be building a classifier which identifies only ONE neighbor (this will be futher explained). \n",
    "- In the case of KNN, it uses a method of determining the distance between 'k' different points in space and using a voting method to classify, but in this case there is just 1 neighbor and hence I have eliminated the need for complexity by keeping the training algorithm very simple. \n",
    "- I will be using the IRIS dataset that's built into scikit learn. Since this dataset is already simplified and has only 4 dimensions (or variables), using the neighbors classifier will be easy. It will be much harder as the number of dimensions increase.\n",
    "- I'll be importing a useful tool (Euclidean Distance : Google it!) from the Scipy (SCIentific PYthon library)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing the dataset from sklearn\n",
    "The Iris Dataset is already loaded in sklearn and more details about iris can be found here (https://en.wikipedia.org/wiki/Iris_flower_data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = iris.data\n",
    "# Iris.data contains the features or independent variables.\n",
    "y = iris.target\n",
    "# Iris.target contains the labels or the dependent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Doing a train-test split by using 50% of the data as our training set\n",
    "\n",
    "The train-test-splitter found in the cross-valiation (now model selection module) of sklearn is a simple but powerful tool to randomly split the data into train and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.20, random_state=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If you have used Machine Learning Classifiers in Python before, remember that there are usually 5 steps involved : \n",
    "- Select the model\n",
    "- Train the model\n",
    "- Fit the model\n",
    "- Predict the outcome\n",
    "- Check the accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Calling the Euclidean Distance and creating a function to call it\n",
    "\n",
    "This works similar to the Pythagorean Theorem but it can work in more than one dimension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.spatial import distance  # Built in function called distance.\n",
    "\n",
    "                                    #Defining the n dimensional distance as euc.\n",
    "def euc(a,b):                       # Lists of numeric features. \n",
    "    return distance.euclidean(a,b)  # Measure and return the distance between 2 points \n",
    "                                    # i.e. the training point and a test point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The Real Deal : Building (Coding up) the classifier\n",
    "\n",
    "The details are explained in comment form line-by-line in the following section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First we implement a class. Classes let you structure code in a specific way.(source --> https://learnpythonthehardway.org/book/ex40.html)\n",
    "\n",
    "class OneNeighborClassifier():                # This 'class' has 2 Methods : Fit and Predict\n",
    "    \n",
    "    #Each step is followed by a comment which explains how the classifier is working \n",
    "    \n",
    "    def fit(self, X_train, y_train):          # Takes features and labels as input\n",
    "        self.X_train = X_train                # Storing the X_train in self.X_train\n",
    "        self.y_train = y_train                # Storing the y_train in self.y_train\n",
    "                                              # This is like the ML classifier will memorize the values \n",
    "        \n",
    "    def predict (self, X_test):               # Receives features from the testing data and returns predictions\n",
    "        predictions = []                      # List of predictions, since X_test is a 2D array or a list of lists.\n",
    "        for row in X_test:                    # Each row contains the features for one testing example\n",
    "            label = self.closest(row)         # We are calling the function that we are creating in the next block\n",
    "                                              # to find the closest training point from the test point\n",
    "            predictions.append(label)         # Add the labels to the predictions list to fill it.\n",
    "        return predictions                    # Return predictions as the output\n",
    "    \n",
    "    def closest(self, row):                   # Create the function closest such that -->\n",
    "        best_dist = euc(row, self.X_train[0]) # Measure the shortest distance a test point and the first train point\n",
    "        best_index = 0                        # Keep track of the index of the train point that is closest\n",
    "        for i in range (1, len(self.X_train)):# Iterate over the different training points\n",
    "            dist = euc(row, self.X_train[i])\n",
    "            if dist < best_dist:              # The moment we find a closer one, we update our variables.\n",
    "                best_dist = dist              # If dist is shorter than best_dist, then its the new best_dist\n",
    "                best_index = i                # Using the index of best_dist to return label of the closest training pt.\n",
    "        return self.y_train[best_index]       # Return that label \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Final Steps\n",
    "#### The classifier is built to utilize the standard pipeline that we use in scikit learn i.e. : \n",
    "- Call the classifier\n",
    "- Fit the model to train it\n",
    "- Predict the model on the test set \n",
    "- Check for accuracy between the real values and the predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_classifier = OneNeighborClassifier()\n",
    "my_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = my_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the classifier is 100.0 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print ('Accuracy of the classifier is', accuracy_score(y_test, pred)*100, '%')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "daf860eab4114424eef43b9e18fa4086445dc785c5c3230227d79cb1b34d6646"
  },
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
